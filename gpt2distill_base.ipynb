{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "import gzip\n",
    "from typing import List\n",
    "import json\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the movie title data and filter US movies to sample 20000 for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhil\\AppData\\Local\\Temp\\ipykernel_25420\\1960824167.py:4: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  movie_df = pd.read_table(f, sep='\\t', na_values=[\"\\\\N\",\"nan\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             titleId  ordering                     title region language  \\\n",
      "16065241   tt1601958         2  The Seven Masks of Volto     US      NaN   \n",
      "14795078  tt15255036         2          Full Time Pimpin     US      NaN   \n",
      "36522495   tt8768044         4      Desire (Chapter Two)     US      NaN   \n",
      "10680532  tt13292924         2        Roadblock and Play     US      NaN   \n",
      "33232      tt0010233         3        Hearts and Flowers     US      NaN   \n",
      "\n",
      "                types attributes  isOriginalTitle  \n",
      "16065241  imdbDisplay        NaN              0.0  \n",
      "14795078  imdbDisplay        NaN              0.0  \n",
      "36522495          dvd        NaN              0.0  \n",
      "10680532  imdbDisplay        NaN              0.0  \n",
      "33232     imdbDisplay        NaN              0.0  \n"
     ]
    }
   ],
   "source": [
    "fname = 'title.akas.tsv.gz'\n",
    "\n",
    "with gzip.open(fname, 'rb') as f:\n",
    "    movie_df = pd.read_table(f, sep='\\t', na_values=[\"\\\\N\",\"nan\"])\n",
    "\n",
    "# print(movie_df.head())\n",
    "\n",
    "movie_df_sampled = movie_df[movie_df['region']==\"US\"].sample(20000)\n",
    "print(movie_df_sampled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics about the movie titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length: 3.4815\n",
      "Std length: 2.4839600942849303\n",
      "Max length 94\n",
      "Max length for model: 10\n"
     ]
    }
   ],
   "source": [
    "movie_titles = movie_df_sampled['title'].tolist()\n",
    "title_lengths = [len(title.split()) for title in movie_titles]\n",
    "\n",
    "mean_length = np.mean(title_lengths)\n",
    "std_length = np.std(title_lengths)\n",
    "print(\"Mean length:\",mean_length)\n",
    "print(\"Std length:\",std_length)\n",
    "print(\"Max length\",max(title_lengths))\n",
    "\n",
    "max_len = int(mean_length + 3*std_length)\n",
    "print(\"Max length for model:\",max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader class\n",
    "\n",
    "- For training, it encodes `<len> ## <word> asfgads <text> asd jjksd lksda`\n",
    "- For testing, it encodes `<len> ## <word> asfgads <text> `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDataset(Dataset):  \n",
    "    def __init__(self, tokenizer, movie_titles: List, max_len: int, dataset_type: str,max_seq_len: int=30) -> None:\n",
    "        self.max_len = max_len\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos = self.tokenizer.eos_token\n",
    "        self.eos_id = self.tokenizer.eos_token_id\n",
    "        self.movies = movie_titles\n",
    "        self.dataset_type = dataset_type\n",
    "        self.result = []\n",
    "        self.populate()\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.result)\n",
    "\n",
    "\n",
    "    def __getitem__(self, item: int) -> torch.Tensor:\n",
    "        return self.result[item]\n",
    "    \n",
    "    def populate(self) -> None:\n",
    "        for movie in self.movies:\n",
    "            movie_words = movie.split()\n",
    "            movie_len = len(movie_words)\n",
    "            if movie_len > 1:\n",
    "                prefix = f\"<len> {movie_len-1} <word> {movie_words[0]} <text> \"\n",
    "                movie = (\" \").join(movie_words[1:])\n",
    "            else:\n",
    "                prefix = f\"<len> {movie_len} <word> movie <text> \"\n",
    "                movie = (\" \").join(movie_words[:])\n",
    "\n",
    "            encoded_prefix = self.tokenizer.encode(prefix)\n",
    "            if self.dataset_type==\"train\":\n",
    "                encoded_movie = self.tokenizer.encode(movie)\n",
    "                if len(encoded_movie)>self.max_len:\n",
    "                    encoded_movie = encoded_movie[:self.max_len]\n",
    "                encoded_input = encoded_prefix + encoded_movie\n",
    "                if len(encoded_input)>self.max_seq_len:\n",
    "                    encoded_input = encoded_input[:self.max_seq_len-1]\n",
    "                padded = encoded_input + [self.eos_id]*(self.max_seq_len-len(encoded_input))\n",
    "            elif self.dataset_type==\"test\":\n",
    "                padded = encoded_prefix\n",
    "            # print(len(padded))\n",
    "            self.result.append(torch.tensor(padded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2DistillMovie(torch.nn.Module):\n",
    "    def __init__(self, device: str, teacher_model: str=None, student_model: str=None):\n",
    "        super().__init__()\n",
    "        self.teacher_model = AutoModelWithLMHead.from_pretrained(teacher_model)\n",
    "        if student_model:\n",
    "            self.student_model = AutoModelWithLMHead.from_pretrained(student_model)\n",
    "        else:\n",
    "            self.student_model = AutoModelWithLMHead.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "        self.teacher_model = self.teacher_model.to(device)\n",
    "        self.student_model = self.student_model.to(device)\n",
    "\n",
    "        self.tokenizer = tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        self.optimizer = optim.AdamW(self.student_model.parameters(), lr=5e-4)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return self.student_model(tensor)\n",
    "\n",
    "    def train(self,train_dataloader, epochs: int, temperature: float=2.0):    \n",
    "        for epoch in range(epochs):\n",
    "            self.student_model.train()\n",
    "            total_loss = 0.0\n",
    "            for idx, batch in enumerate(train_dataloader):\n",
    "                self.optimizer.zero_grad()\n",
    "                batch = batch.to(device)\n",
    "                with torch.no_grad():\n",
    "                    teacher_outputs = self.teacher_model(batch)\n",
    "                    logits_teacher = teacher_outputs.logits\n",
    "\n",
    "                outputs = self.student_model(batch, labels=batch)\n",
    "                logits_student = outputs.logits\n",
    "\n",
    "                loss = outputs.loss + self.distillation_loss(logits_student, logits_teacher, temperature=temperature)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                # if idx % 100 == 0:\n",
    "                #         print(\"loss: %f, %d\"%(loss, idx))\n",
    "\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "\n",
    "    def distillation_loss(self,logits_student: torch.Tensor, logits_teacher: torch.Tensor, temperature: float=2.0) -> torch.Tensor:\n",
    "\n",
    "        p_student = torch.nn.functional.log_softmax(logits_student / temperature, dim=-1)\n",
    "        p_teacher = torch.nn.functional.softmax(logits_teacher / temperature, dim=-1)\n",
    "        loss = torch.nn.functional.kl_div(p_student, p_teacher, reduction='batchmean') * (temperature** 2)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def save(self, filepath: str=\"distilled_model/\") -> None:\n",
    "        self.student_model.save_pretrained(save_directory=filepath)\n",
    "        self.tokenizer.save_vocabulary(save_directory=filepath)\n",
    "        \n",
    "    def topk(self,probs: torch.Tensor, k: int=5) -> int:\n",
    "        probs = torch.softmax(probs, dim= -1)\n",
    "\n",
    "        token_probs, topIx = torch.topk(probs, k=k)\n",
    "        token_probs = token_probs / torch.sum(token_probs)\n",
    "        token_probs = token_probs.cpu().detach().numpy()\n",
    "\n",
    "        choice = np.random.choice(k, 1, p = token_probs)\n",
    "        token_id = topIx[choice][0]\n",
    "\n",
    "        return int(token_id)\n",
    "    \n",
    "    def inference(self, init_token: torch.Tensor, max_length: int=10) -> str:\n",
    "\n",
    "        sequence = init_token.numpy().tolist()\n",
    "        init_input = init_token.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            output = self.student_model(init_input)\n",
    "            logits = output.logits[0,-1]\n",
    "\n",
    "            sequence.append(self.topk(logits))\n",
    "\n",
    "            for i in range(max_length):\n",
    "                inp = torch.tensor(sequence).unsqueeze(0).to(device)\n",
    "                output = self.student_model(inp)\n",
    "                logits = output.logits[0,-1]\n",
    "                res_id = self.topk(logits)\n",
    "\n",
    "                if res_id == self.tokenizer.eos_token_id:\n",
    "                    return self.tokenizer.decode(sequence)\n",
    "                else: \n",
    "                    sequence.append(res_id)\n",
    "\n",
    "        return self.tokenizer.decode(sequence)\n",
    "\n",
    "    def eval(self, test_dataset) -> None:\n",
    "        results = []\n",
    "        within_max_len = 0\n",
    "        within_req_len = 0\n",
    "        equal_req_len = 0\n",
    "        req_len = []\n",
    "        gen_len = []\n",
    "        for inp in test_dataset:\n",
    "            ret_seq = self.inference(inp).strip()\n",
    "            results.append(ret_seq)\n",
    "            true_len = int(ret_seq.split(\"<text>\")[0].split(\" \")[1])\n",
    "            output = ret_seq.split(\"<text>\")[1].split(\" \")[1:]\n",
    "            # print(req_len,len(output),output)\n",
    "            if len(output)<=max_len:\n",
    "                within_max_len+=1\n",
    "            if len(output)<=true_len:\n",
    "                within_req_len+=1\n",
    "                if len(output)==true_len:\n",
    "                    equal_req_len+=1\n",
    "            req_len.append(true_len)\n",
    "            gen_len.append(len(output))\n",
    "            \n",
    "        \n",
    "        result_json = {\"within_max_len\":within_max_len/len(test_dataset),\n",
    "                        \"within_req_len\": within_req_len/len(test_dataset),\n",
    "                        \"equal_req_len\":equal_req_len/len(test_dataset),\n",
    "                        \"MSE_genvreq\":mean_squared_error(req_len,gen_len),\n",
    "                        \"gen_results\":results}\n",
    "                        \n",
    "        json_file_path = \"eval_student_results.json\"\n",
    "\n",
    "        with open(json_file_path, \"w\") as json_file:\n",
    "            json.dump(result_json, json_file, indent=4)\n",
    "        \n",
    "        print(f\"Output within max seq length: {within_max_len/len(test_dataset)}\")\n",
    "        print(f\"Output within req seq length: {within_req_len/len(test_dataset)}\")\n",
    "        print(f\"Output equal req seq length: {equal_req_len/len(test_dataset)}\")\n",
    "        print(f\"MSE req vs gen seq length: {mean_squared_error(req_len,gen_len)}\")\n",
    "        print(\"-\"*20)\n",
    "        print(results[:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1468: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gpt2distill = GPT2DistillMovie(device,teacher_model=\"model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the tokenizer and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "dataset = MovieDataset(gpt2distill.tokenizer, movie_titles, max_len, dataset_type=\"train\")\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 36.21026574707031\n",
      "Epoch 2/20, Loss: 21.448064233398437\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gpt2distill\u001b[38;5;241m.\u001b[39mtrain(train_dataloader\u001b[38;5;241m=\u001b[39mdataloader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 39\u001b[0m, in \u001b[0;36mGPT2DistillMovie.train\u001b[1;34m(self, train_dataloader, epochs, temperature)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# if idx % 100 == 0:\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m#         print(\"loss: %f, %d\"%(loss, idx))\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gpt2distill.train(train_dataloader=dataloader, epochs=20,temperature=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2distill.save(\"distilled_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2distill = GPT2DistillMovie(device,teacher_model=\"model/\",student_model=\"distilled_model/\")\n",
    "\n",
    "movie_test = movie_df[movie_df['region']==\"US\"].sample(1000)\n",
    "movie_test = movie_test['title'].tolist()\n",
    "test_dataset = MovieDataset(gpt2distill.tokenizer, movie_test, max_len, dataset_type=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abhil\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Output within max seq length: 1.0\n",
      "Output within req seq length: 0.945\n",
      "Output equal req seq length: 0.789\n",
      "MSE req vs gen seq length: 0.974\n",
      "--------------------\n",
      "['<len> 3 <word> Rachael <text> Rayal/Alex Eden/CyHi', '<len> 3 <word> Melanie <text> Tree/John Pugh/Regina Deme', '<len> 2 <word> So <text> Long, Long', '<len> 1 <word> L.A. <text> Muse', '<len> 3 <word> Scott <text> Free: Words', \"<len> 3 <word> Didn't <text> I Tweet\", '<len> 5 <word> Broken <text> Sharts and Broken Dreams', '<len> 5 <word> Zeta <text> No. 13: The First Edition', '<len> 2 <word> Lil <text> Y-Lane/Jean', '<len> 3 <word> Mystery <text> on the Trail']\n"
     ]
    }
   ],
   "source": [
    "gpt2distill.eval(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
