{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "import gzip\n",
    "from typing import List\n",
    "import json\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the movie title data and filter US movies to sample 20000 for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhil\\AppData\\Local\\Temp\\ipykernel_17828\\1960824167.py:4: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  movie_df = pd.read_table(f, sep='\\t', na_values=[\"\\\\N\",\"nan\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             titleId  ordering                           title region  \\\n",
      "28073451  tt30826278         1  King of Style: Micheal Jackson     US   \n",
      "7263107   tt11703044         1                    Gregory Blue     US   \n",
      "1854642    tt0368868         1               A Mission to Kill     US   \n",
      "986485     tt0112782         4           The Dallas Connection     US   \n",
      "31034484   tt5061158         1                  Truth or Scare     US   \n",
      "\n",
      "         language        types attributes  isOriginalTitle  \n",
      "28073451      NaN          NaN        NaN              0.0  \n",
      "7263107       NaN          NaN        NaN              0.0  \n",
      "1854642       NaN  imdbDisplay        NaN              0.0  \n",
      "986485        NaN  imdbDisplay        NaN              0.0  \n",
      "31034484      NaN          NaN        NaN              0.0  \n"
     ]
    }
   ],
   "source": [
    "fname = 'title.akas.tsv.gz'\n",
    "\n",
    "with gzip.open(fname, 'rb') as f:\n",
    "    movie_df = pd.read_table(f, sep='\\t', na_values=[\"\\\\N\",\"nan\"])\n",
    "\n",
    "# print(movie_df.head())\n",
    "\n",
    "movie_df_sampled = movie_df[movie_df['region']==\"US\"].sample(20000)\n",
    "print(movie_df_sampled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics about the movie titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length: 3.49345\n",
      "Std length: 2.3171657466612094\n",
      "Max length 36\n",
      "Max length for model: 10\n"
     ]
    }
   ],
   "source": [
    "movie_titles = movie_df_sampled['title'].tolist()\n",
    "title_lengths = [len(title.split()) for title in movie_titles]\n",
    "\n",
    "mean_length = np.mean(title_lengths)\n",
    "std_length = np.std(title_lengths)\n",
    "print(\"Mean length:\",mean_length)\n",
    "print(\"Std length:\",std_length)\n",
    "print(\"Max length\",max(title_lengths))\n",
    "\n",
    "max_len = int(mean_length + 3*std_length)\n",
    "print(\"Max length for model:\",max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader class\n",
    "\n",
    "- For training, it encodes `<len> ## <word> asfgads <text> asd jjksd lksda`\n",
    "- For testing, it encodes `<len> ## <word> asfgads <text> `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDataset(Dataset):  \n",
    "    def __init__(self, tokenizer, movie_titles: List, max_len: int, dataset_type: str,max_seq_len: int=30) -> None:\n",
    "        self.max_len = max_len\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos = self.tokenizer.eos_token\n",
    "        self.eos_id = self.tokenizer.eos_token_id\n",
    "        self.movies = movie_titles\n",
    "        self.dataset_type = dataset_type\n",
    "        self.result = []\n",
    "        self.populate()\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.result)\n",
    "\n",
    "\n",
    "    def __getitem__(self, item: int) -> torch.Tensor:\n",
    "        return self.result[item]\n",
    "    \n",
    "    def populate(self) -> None:\n",
    "        for movie in self.movies:\n",
    "            movie_words = movie.split()\n",
    "            movie_len = len(movie_words)\n",
    "            if movie_len > 1:\n",
    "                prefix = f\"<len> {movie_len-1} <word> {movie_words[0]} <text> \"\n",
    "                movie = (\" \").join(movie_words[1:])\n",
    "            else:\n",
    "                prefix = f\"<len> {movie_len} <word> movie <text> \"\n",
    "                movie = (\" \").join(movie_words[:])\n",
    "\n",
    "            encoded_prefix = self.tokenizer.encode(prefix)\n",
    "            if self.dataset_type==\"train\":\n",
    "                encoded_movie = self.tokenizer.encode(movie)\n",
    "                if len(encoded_movie)>self.max_len:\n",
    "                    encoded_movie = encoded_movie[:self.max_len]\n",
    "                encoded_input = encoded_prefix + encoded_movie\n",
    "                if len(encoded_input)>self.max_seq_len:\n",
    "                    encoded_input = encoded_input[:self.max_seq_len-1]\n",
    "                padded = encoded_input + [self.eos_id]*(self.max_seq_len-len(encoded_input))\n",
    "            elif self.dataset_type==\"test\":\n",
    "                padded = encoded_prefix\n",
    "            # print(len(padded))\n",
    "            self.result.append(torch.tensor(padded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Movie(torch.nn.Module):\n",
    "    def __init__(self, device: str, pretrained_model: str=None):\n",
    "        super().__init__()\n",
    "        self.model = None\n",
    "        if pretrained_model:\n",
    "            self.model = AutoModelWithLMHead.from_pretrained(pretrained_model)\n",
    "        else:\n",
    "            self.model = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
    "        self.model = self.model.to(device)\n",
    "        self.tokenizer = tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=5e-4)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(tensor)\n",
    "\n",
    "    def train(self,train_dataloader, epochs: int) -> None:\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for idx, batch in enumerate(train_dataloader):\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    self.optimizer.zero_grad()\n",
    "                    batch = batch.to(device)\n",
    "                    output = self.model(batch, labels=batch)\n",
    "                    loss = output[0]\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    # if idx % 100 == 0:\n",
    "                    #     print(\"loss: %f, %d\"%(loss, idx))\n",
    "                    total_loss += loss.item()\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "\n",
    "    def save(self,filepath: str=\"model/\") -> None:\n",
    "        self.model.save_pretrained(save_directory=filepath)\n",
    "        self.tokenizer.save_vocabulary(save_directory=filepath)\n",
    "        \n",
    "    def topk(self,probs: torch.Tensor, k: int=5) -> int:\n",
    "        probs = torch.softmax(probs, dim= -1)\n",
    "\n",
    "        tokensProb, topIx = torch.topk(probs, k=k)\n",
    "        tokensProb = tokensProb / torch.sum(tokensProb)\n",
    "        tokensProb = tokensProb.cpu().detach().numpy()\n",
    "\n",
    "        choice = np.random.choice(k, 1, p = tokensProb)\n",
    "        tokenId = topIx[choice][0]\n",
    "\n",
    "        return int(tokenId)\n",
    "    \n",
    "    def inference(self, init_token: torch.Tensor, max_length: int=10) -> str:\n",
    "\n",
    "        sequence = init_token.numpy().tolist()\n",
    "        init_input = init_token.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            output = self.model(init_input)\n",
    "            logits = output.logits[0,-1]\n",
    "\n",
    "            sequence.append(self.topk(logits))\n",
    "\n",
    "            for i in range(max_length):\n",
    "                inp = torch.tensor(sequence).unsqueeze(0).to(device)\n",
    "                output = self.model(inp)\n",
    "                logits = output.logits[0,-1]\n",
    "                res_id = self.topk(logits)\n",
    "\n",
    "                if res_id == self.tokenizer.eos_token_id:\n",
    "                    return self.tokenizer.decode(sequence)\n",
    "                else: \n",
    "                    sequence.append(res_id)\n",
    "\n",
    "        return self.tokenizer.decode(sequence)\n",
    "\n",
    "    def eval(self,test_dataset) -> None:\n",
    "        results = []\n",
    "        within_max_len = 0\n",
    "        within_req_len = 0\n",
    "        equal_req_len = 0\n",
    "        req_len = []\n",
    "        gen_len = []\n",
    "        for inp in test_dataset:\n",
    "            ret_seq = self.inference(inp).strip()\n",
    "            results.append(ret_seq)\n",
    "            true_len = int(ret_seq.split(\"<text>\")[0].split(\" \")[1])\n",
    "            output = ret_seq.split(\"<text>\")[1].split(\" \")[1:]\n",
    "            # print(req_len,len(output),output)\n",
    "            if len(output)<=max_len:\n",
    "                within_max_len+=1\n",
    "            if len(output)<=true_len:\n",
    "                within_req_len+=1\n",
    "                if len(output)==true_len:\n",
    "                    equal_req_len+=1\n",
    "            req_len.append(true_len)\n",
    "            gen_len.append(len(output))\n",
    "            \n",
    "        \n",
    "        result_json = {\"within_max_len\":within_max_len/len(test_dataset),\n",
    "                        \"within_req_len\": within_req_len/len(test_dataset),\n",
    "                        \"equal_req_len\":equal_req_len/len(test_dataset),\n",
    "                        \"MSE_genvreq\":mean_squared_error(req_len,gen_len),\n",
    "                        \"gen_results\":results}\n",
    "                        \n",
    "        json_file_path = \"eval_results.json\"\n",
    "\n",
    "        with open(json_file_path, \"w\") as json_file:\n",
    "            json.dump(result_json, json_file, indent=4)\n",
    "        \n",
    "        print(f\"Output within max seq length: {within_max_len/len(test_dataset)}\")\n",
    "        print(f\"Output within req seq length: {within_req_len/len(test_dataset)}\")\n",
    "        print(f\"Output equal req seq length: {equal_req_len/len(test_dataset)}\")\n",
    "        print(f\"MSE req vs gen seq length: {mean_squared_error(req_len,gen_len)}\")\n",
    "        print(\"-\"*20)\n",
    "        print(results[:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1468: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gpt2 = GPT2Movie(device,\"model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the tokenizer and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MovieDataset(gpt2.tokenizer, movie_titles, max_len, dataset_type=\"train\")\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.1102415719032288\n"
     ]
    }
   ],
   "source": [
    "gpt2.train(train_dataloader=dataloader, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.save(\"model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_test = movie_df[movie_df['region']==\"US\"].sample(1000)\n",
    "movie_test = movie_test['title'].tolist()\n",
    "test_dataset = MovieDataset(gpt2.tokenizer, movie_test, max_len, dataset_type=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output within max seq length: 1.0\n",
      "Output within req seq length: 0.946\n",
      "Output equal req seq length: 0.829\n",
      "MSE req vs gen seq length: 1.386\n",
      "--------------------\n",
      "['<len> 1 <word> The <text> Wager', '<len> 2 <word> The <text> Wicked Ones', '<len> 1 <word> Global <text> Addiction', '<len> 3 <word> Chasing <text> a Booming Market', '<len> 6 <word> Walt <text> Disney World Christmas Day Parade', '<len> 5 <word> John <text> Brenkus Presents the GOAT', '<len> 1 <word> movie <text> Honeymooniacs', '<len> 3 <word> Tucker, <text> the Tucker Film', \"<len> 2 <word> Her <text> Master's Voice\", '<len> 5 <word> Gora: <text> Lad My Very Adventures']\n"
     ]
    }
   ],
   "source": [
    "gpt2.eval(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
